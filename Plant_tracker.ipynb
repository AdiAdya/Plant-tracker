{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbJLmORDR4bt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from xml.etree import ElementTree\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from comet_ml import Experiment as Comet\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from albumentations import HorizontalFlip, ShiftScaleRotate, RandomBrightnessContrast, Compose\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Data(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class to load and process the Plant Tracer data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: Path\n",
    "        Path to location of frames and annotations\n",
    "        The videos are expected as ZIP file containing all the frames\n",
    "        The annotations are expected in PASCAL_VOC format\n",
    "    target_size: int\n",
    "        The size of image to use when training the model\n",
    "        The image will be of size (@target_size x @target_size)\n",
    "    transforms: bool\n",
    "        Apply augmentation transformations on the image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Path, target_size: int, transforms: bool = True):\n",
    "        self.target_size = target_size\n",
    "        self.transforms = transforms\n",
    "        all_frames = list(path.glob('*.zip'))\n",
    "        all_annotations = list(path.glob('*.xml'))\n",
    "        self.video_annotations = [self.load_annotations(a) for a in\n",
    "                                  tqdm(all_annotations, desc='Loading Annotations')]\n",
    "        self.video_frames = [self.load_frames(f) for f in tqdm(all_frames, desc='Loading Video Frames')]\n",
    "        self.__sanity_check__()\n",
    "        if self.transforms:\n",
    "            self.prev_hflip = Compose([HorizontalFlip(p=1)])\n",
    "            self.prev_ssr = Compose([ShiftScaleRotate(p=1)])\n",
    "            self.prev_bri = Compose([RandomBrightnessContrast(p=1)])\n",
    "            self.curr_hflip = Compose([HorizontalFlip(p=1)],\n",
    "                                      bbox_params={'format': 'pascal_voc', 'label_fields': ['category_id']})\n",
    "            self.curr_ssr = Compose([ShiftScaleRotate(p=1)],\n",
    "                                    bbox_params={'format': 'pascal_voc', 'label_fields': ['category_id']})\n",
    "            self.curr_bri = Compose([RandomBrightnessContrast(p=1)],\n",
    "                                    bbox_params={'format': 'pascal_voc', 'label_fields': ['category_id']})\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculate the total length of dataset\n",
    "        Returns\n",
    "        -------\n",
    "            int: Length of dataset\n",
    "        \"\"\"\n",
    "        return sum([len(videos) - 1 for videos in self.video_frames])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve a random item from the dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray: The previous frame cropped based on annotation\n",
    "            ndarray: The current frame cropped based on previous annotation\n",
    "            ndarray: The current annotation rescaled\n",
    "            list: The scale used to crop resize the frames\n",
    "            list: The amount of crop performed on the frames\n",
    "        \"\"\"\n",
    "        # Select a random video and then, select a random frame in that video\n",
    "        np.random.seed(seed=index)\n",
    "        vi = np.random.randint(low=0, high=len(self.video_frames) - 1, size=1)[0]\n",
    "        fi = np.random.randint(low=1, high=len(self.video_frames[vi]), size=1)[0]\n",
    "        np.random.seed(seed=None)\n",
    "        previous_frame = self.video_frames[vi][fi - 1]\n",
    "        current_frame = self.video_frames[vi][fi]\n",
    "        previous_annotation = self.video_annotations[vi][fi - 1]\n",
    "        current_annotation = self.video_annotations[vi][fi]\n",
    "        return self.make_crops(previous_frame, current_frame, previous_annotation, current_annotation)\n",
    "\n",
    "    def __sanity_check__(self):\n",
    "        \"\"\"\n",
    "        Make a check to see if number of videos and annotations match\n",
    "        Raises\n",
    "        -------\n",
    "        ValueError\n",
    "            If the number of video frames does not match the number of annotations, or\n",
    "            if each video does not have its respective annotations\n",
    "        \"\"\"\n",
    "        if len(self.video_annotations) != len(self.video_frames):\n",
    "            raise ValueError('Sizes of annotations and videos do not match')\n",
    "        # Also make a check to see if number of frames in each video and number of annotations for each video match\n",
    "        else:\n",
    "            for i, d in enumerate(zip(self.video_annotations, self.video_frames)):\n",
    "                if d[0].shape[0] != d[1].shape[0]:\n",
    "                    raise ValueError('Sizes of annotations and videos do not match in {}'.format(i + 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_pickles(path: Path):\n",
    "        \"\"\"\n",
    "        Load the video frames and annotations using pickle files\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: Path\n",
    "            Path to location of pickle file which contains the video frames and annotations\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Video annotations and video frames extracted from the pickle file\n",
    "        \"\"\"\n",
    "        pickles = list(path.glob('*.pkl'))\n",
    "        video_annotations = []\n",
    "        video_frames = []\n",
    "        for p in tqdm(pickles, desc='Loading Pickles'):\n",
    "            with open(p, 'rb') as pkl:\n",
    "                save_dict = pickle.load(pkl)\n",
    "                video_annotations.append(save_dict['annotations'])\n",
    "                video_frames.append(save_dict['frames'])\n",
    "        return video_annotations, video_frames\n",
    "\n",
    "    @staticmethod\n",
    "    def load_annotations(path: Path):\n",
    "        \"\"\"\n",
    "        Load the annotations from XML file\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: Path\n",
    "            Path pointing to the XML file which contains the annotations\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray: The annotations loaded as an array\n",
    "        \"\"\"\n",
    "        #\n",
    "        root = ElementTree.parse(path).getroot()\n",
    "        polygons = root[3].findall('polygon')\n",
    "        buffer = np.empty((len(polygons), 4), np.int)\n",
    "        for i, polygon in enumerate(polygons):\n",
    "            pts = polygon.findall('pt')\n",
    "            buffer[i][0] = int(pts[1][0].text)  # top-left x\n",
    "            buffer[i][1] = int(pts[1][1].text)  # top-left y\n",
    "            buffer[i][2] = int(pts[3][0].text)  # bottom-right x\n",
    "            buffer[i][3] = int(pts[3][1].text)  # bottom-right y\n",
    "        return buffer\n",
    "\n",
    "    @staticmethod\n",
    "    def load_frames(path: Path):\n",
    "        \"\"\"\n",
    "        Load the frames from the ZIP file\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: Path\n",
    "            Path pointing to the ZIP file which contains the video frames\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray: The video frames loaded as an array\n",
    "        \"\"\"\n",
    "        zip_file = ZipFile(path)\n",
    "        names = zip_file.namelist()\n",
    "        buf = [cv2.imdecode(np.frombuffer(BytesIO(zip_file.open(name).read()).read(), np.uint8), 1) for name in names]\n",
    "        return np.array(buf)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_video(path: Path):\n",
    "        \"\"\"\n",
    "        Load video frames from video file\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: Path\n",
    "            Path pointing to the video file which has to be loaded\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray: The video frames loaded as an array\n",
    "        \"\"\"\n",
    "        #\n",
    "        cap = cv2.VideoCapture(str(path.resolve()))\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        buffer = np.empty((frame_count, frame_height, frame_width, 3), np.uint8)\n",
    "        fc = 0\n",
    "        ret = True\n",
    "        while fc < frame_count and ret:\n",
    "            ret, buffer[fc] = cap.read()\n",
    "            fc += 1\n",
    "        cap.release()\n",
    "        return buffer\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_channels(image: np.ndarray, channel_first: bool = False, channel_last: bool = False):\n",
    "        \"\"\"\n",
    "        Converts image to channel first or channel last format\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image: ndarray\n",
    "            The image array\n",
    "        channel_first: bool\n",
    "            Set this to True to convert from channel last to channel first\n",
    "        channel_last: bool\n",
    "            Set this to True to convert from channel first to channel last\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray: Channel converted image\n",
    "        \"\"\"\n",
    "        return np.moveaxis(image, -1, 0) if channel_first else (np.moveaxis(image, 0, -1) if channel_last else image)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bbox(preds: np.ndarray, target_size: int, scale: list, crop: list):\n",
    "        \"\"\"\n",
    "        Recovers the bounding boxes after prediction\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        preds: ndarray\n",
    "            The predicted bounding box coordinates\n",
    "        target_size: int\n",
    "            The image size\n",
    "        scale: list\n",
    "            The scale used when rescaling the bounding boxes\n",
    "        crop: list\n",
    "            The crop used when resizing the bounding boxes\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray: Recovered bounding box\n",
    "        \"\"\"\n",
    "        preds = np.divide(preds, 10)\n",
    "        preds = np.multiply(preds, target_size)\n",
    "        preds = np.divide(preds, scale * 2)\n",
    "        preds = np.add(preds, crop * 2)\n",
    "        return preds.astype(int)\n",
    "\n",
    "    def make_crops(self, previous_frame: np.ndarray,\n",
    "                   current_frame: np.ndarray,\n",
    "                   previous_annotation: np.ndarray,\n",
    "                   current_annotation: np.ndarray,\n",
    "                   validate: bool = False):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        previous_frame: ndarray\n",
    "            The previous frame of the selected video frame\n",
    "        current_frame: ndarray\n",
    "            The current video frame\n",
    "        previous_annotation: ndarray\n",
    "            The annotation for the previous video frame\n",
    "        current_annotation: ndarray\n",
    "            The annotation for the current video frame\n",
    "        validate: bool, default = False\n",
    "            Will not perform image transformation if set to True\n",
    "        Returns\n",
    "        -------\n",
    "            ndarray: The previous frame cropped based on annotation\n",
    "            ndarray: The current frame cropped based on previous annotation\n",
    "            ndarray: The current annotation rescaled\n",
    "            list: The scale used to crop resize the frames\n",
    "            list: The amount of crop performed on the frames\n",
    "        \"\"\"\n",
    "        # Clip the bounding box to makes it lies inside the image\n",
    "        previous_annotation[0] = np.clip(previous_annotation[0], 0, previous_frame.shape[0])\n",
    "        previous_annotation[1] = np.clip(previous_annotation[1], 0, previous_frame.shape[1])\n",
    "        previous_annotation[2] = np.clip(previous_annotation[2], 0, previous_frame.shape[0])\n",
    "        previous_annotation[3] = np.clip(previous_annotation[3], 0, previous_frame.shape[1])\n",
    "\n",
    "        center_x = int((previous_annotation[0] + previous_annotation[2]) / 2)\n",
    "        center_y = int((previous_annotation[1] + previous_annotation[3]) / 2)\n",
    "        width = abs(previous_annotation[2] - previous_annotation[0])\n",
    "        height = abs(previous_annotation[3] - previous_annotation[1])\n",
    "\n",
    "        # Create a crop window of size 7 x max(height, width) of the image\n",
    "        crop_size = np.clip(7 * max(width, height), 10, 120)\n",
    "        top = np.clip(center_x + crop_size, 0, previous_frame.shape[0])\n",
    "        left = np.clip(center_y - crop_size, 0, previous_frame.shape[1])\n",
    "        bottom = np.clip(center_x - crop_size, 0, previous_frame.shape[0])\n",
    "        right = np.clip(center_y + crop_size, 0, previous_frame.shape[1])\n",
    "        crop = [bottom, left]\n",
    "\n",
    "        # Generate the cropped images\n",
    "        previous_cropped = previous_frame[left: right, bottom: top, :]\n",
    "        current_cropped = current_frame[left: right, bottom: top, :]\n",
    "\n",
    "        # Calculate the scale needed to resize the image to :target_size:\n",
    "        scale = np.divide(self.target_size, current_cropped.shape[:-1]).tolist()\n",
    "        previous_cropped = cv2.resize(previous_cropped, (self.target_size, self.target_size))\n",
    "        current_cropped = cv2.resize(current_cropped, (self.target_size, self.target_size))\n",
    "\n",
    "        # Scale and crop the bounding box appropriately\n",
    "        bbox = np.subtract(current_annotation, crop * 2)\n",
    "        bbox = np.multiply(bbox, scale * 2)\n",
    "        bbox = np.divide(bbox, self.target_size)\n",
    "\n",
    "        # Apply transformations\n",
    "        if not validate:\n",
    "            if self.transforms:\n",
    "                try:\n",
    "                    x_min, y_max, x_max, y_min = bbox\n",
    "                    bbox = np.array([x_min, y_min, x_max, y_max])\n",
    "                    previous_augmented = {'image': previous_cropped}\n",
    "                    current_augmented = {'image': current_cropped, 'bboxes': [bbox], 'category_id': [0]}\n",
    "                    if np.random.random() > 0.5:\n",
    "                        try:\n",
    "                            previous_augmented = self.prev_hflip(**previous_augmented)\n",
    "                            current_augmented = self.curr_hflip(**current_augmented)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    if np.random.random() > 0.5:\n",
    "                        try:\n",
    "                            previous_augmented = self.prev_ssr(**previous_augmented)\n",
    "                            current_augmented = self.curr_ssr(**current_augmented)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    if np.random.random() > 0.5:\n",
    "                        try:\n",
    "                            previous_augmented = self.prev_bri(**previous_augmented)\n",
    "                            current_augmented = self.curr_bri(**current_augmented)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    previous_cropped = previous_augmented['image']\n",
    "                    current_cropped = current_augmented['image']\n",
    "                    x_min, y_min, x_max, y_max = bbox\n",
    "                    bbox = np.array([x_min, y_max, x_max, y_min])\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "        # Convert images from channel last to channel first format\n",
    "        previous_cropped = self.convert_channels(previous_cropped, channel_first=True)\n",
    "        current_cropped = self.convert_channels(current_cropped, channel_first=True)\n",
    "\n",
    "        # Multiply the bounding box by 10\n",
    "        bbox = np.multiply(bbox, 10)\n",
    "        return previous_cropped, current_cropped, bbox, scale, crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s0CncuucBqv"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GoTurnRemix(nn.Module):\n",
    "    \"\"\"\n",
    "        Create a model based on GOTURN. The GOTURN architecture used a CaffeNet while GoTurnRemix uses AlexNet.\n",
    "        The rest of the architecture is the similar to GOTURN. A PyTorch implementation of GOTURN can be found at:\n",
    "\n",
    "        https://github.com/aakaashjois/PyTorch-GOTURN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoTurnRemix, self).__init__()\n",
    "        # Load an AlexNet model pretrained on ImageNet\n",
    "        self.features = nn.Sequential(*list(models.alexnet(pretrained=True).children())[:-1])\n",
    "        # Freeze the pretrained layers\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4),\n",
    "        )\n",
    "        # Initialize the biases of the Linear layers to 1\n",
    "        # Initialize weights to a normal distribution with 0 mean and 0.005 standard deviation\n",
    "        for m in self.regressor.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.bias.data.fill_(1)\n",
    "                m.weight.data.normal_(0, 0.005)\n",
    "\n",
    "    def forward(self, previous, current):\n",
    "        previous_features = self.features(previous)\n",
    "        current_features = self.features(current)\n",
    "        # Flatten, concatenate and pass to regressor the features\n",
    "        return self.regressor(torch.cat((previous_features.view(previous_features.size(0), 256 * 6 * 6),\n",
    "                                          current_features.view(current_features.size(0), 256 * 6 * 6)), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E2CoKuFR_R8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Experiment:\n",
    "    \"\"\"\n",
    "        A helper class to facilitate the training and validation procedure of the GoTurnRemix model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        learning_rate: float\n",
    "            Learning rate to train the model. The optimizer is SGD and the loss is L1 Loss\n",
    "        image_size: int\n",
    "            The size of the input image. This has to be fixed before the data is created\n",
    "        data_path: Path\n",
    "            Path to the data folder. If the folder name includes \"pickle\", then the data saved as pickles are loaded\n",
    "        augment: bool\n",
    "            Perform augmentation on the images before training\n",
    "        logs_path: Path\n",
    "            Path to save the validation predictions at the end of each epoch\n",
    "        models_path: Path\n",
    "            Path to save the model state at the end of each epoch\n",
    "        save_name: str\n",
    "            Name of the folder in which the logs and models are saved. If not provided, the current datetime is used\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 learning_rate: float,\n",
    "                 image_size: int,\n",
    "                 data_path: Path,\n",
    "                 augment: bool = True,\n",
    "                 logs_path: Path = None,\n",
    "                 models_path: Path = None,\n",
    "                 save_name: str = None,\n",
    "                 comet_api: str = None):\n",
    "        self.image_size = image_size\n",
    "        self.logs_path = logs_path\n",
    "        self.models_path = models_path\n",
    "        self.model = GoTurnRemix()\n",
    "#         self.model.cuda()\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "        self.optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()), lr=learning_rate)\n",
    "        self.model_name = str(datetime.datetime.now()).split('.')[0].replace(':', '-').replace(' ', '-')\n",
    "        self.model_name = save_name if save_name else self.model_name\n",
    "        self.augment = augment\n",
    "        self.data = Data(data_path, target_size=self.image_size, transforms=augment)\n",
    "        self.comet = None\n",
    "        if comet_api:\n",
    "            self.comet = Comet(api_key=comet_api)\n",
    "            self.comet.log_parameter('learning_rate', learning_rate)\n",
    "            self.comet.log_parameter('image_size', image_size)\n",
    "            self.comet.log_parameter('augment', augment)\n",
    "\n",
    "    def __train_step__(self, data):\n",
    "        \"\"\"\n",
    "        Performs one step of the training procedure\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            data obtained from @Data.__getitem__\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "           Loss at the end of training step\n",
    "        \"\"\"\n",
    "        if self.comet:\n",
    "            self.comet.train()\n",
    "        previous_cropped, current_cropped, bbox, scale, crop = data\n",
    "        previous_cropped = torch.div(previous_cropped, 255).float()\n",
    "        current_cropped = torch.div(current_cropped, 255).float()\n",
    "        previous_cropped = torch.autograd.Variable(previous_cropped, requires_grad=True)\n",
    "        current_cropped = torch.autograd.Variable(current_cropped, requires_grad=True)\n",
    "        bbox = bbox.requires_grad_(True).float()\n",
    "        self.optimizer.zero_grad()\n",
    "        preds = self.model(previous_cropped, current_cropped)\n",
    "\n",
    "        del previous_cropped\n",
    "        del current_cropped\n",
    "        gc.collect()\n",
    "\n",
    "        loss = self.criterion(preds, bbox)\n",
    "        if self.comet:\n",
    "            self.comet.log_metric('loss', loss)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def __test__(self):\n",
    "        \"\"\"\n",
    "        Test tracking of the model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Test loss and test predictions\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        if self.comet:\n",
    "            self.comet.test()\n",
    "        self.model.eval()\n",
    "        test_preds = []\n",
    "        test_loss = []\n",
    "        video_frames = self.data.video_frames[-1]\n",
    "        video_annotations = self.data.video_annotations[-1]\n",
    "        p_a = video_annotations[0]\n",
    "        p_f = video_frames[0]\n",
    "        test_preds.append(p_a)\n",
    "\n",
    "        for i in tqdm(range(1, len(video_annotations)), desc='Validating'):\n",
    "            c_a = video_annotations[i]\n",
    "            c_f = video_frames[i]\n",
    "            p_c, c_c, bbox, scale, crop = self.data.make_crops(p_f, c_f, p_a, c_a)\n",
    "            p_c = torch.div(torch.from_numpy(p_c), 255).unsqueeze(0).float()\n",
    "            c_c = torch.div(torch.from_numpy(c_c), 255).unsqueeze(0).float()\n",
    "            bbox = torch.tensor(bbox, requires_grad=False).float()\n",
    "            preds = self.model(p_c, c_c)\n",
    "\n",
    "            del p_c\n",
    "            del c_c\n",
    "            gc.collect()\n",
    "\n",
    "            loss = torch.nn.functional.l1_loss(preds, bbox)\n",
    "            if self.comet:\n",
    "                self.comet.log_metric('val_loss', loss)\n",
    "            test_loss.append(loss.item())\n",
    "            preds = self.data.get_bbox(preds.cpu().detach().numpy()[0], self.image_size, scale, crop)\n",
    "            test_preds.append(preds)\n",
    "            p_a = preds\n",
    "            p_f = c_f\n",
    "        return test_loss, test_preds\n",
    "\n",
    "    def __validate__(self):\n",
    "        \"\"\"\n",
    "        Performs validation on the model\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Validation loss and validation predictions\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode\n",
    "        if self.comet:\n",
    "            self.comet.validate()\n",
    "        self.model.eval()\n",
    "        validation_preds = []\n",
    "        validation_loss = []\n",
    "        video_frames = self.data.video_frames[-1]\n",
    "        video_annotations = self.data.video_annotations[-1]\n",
    "        p_a = video_annotations[0]\n",
    "        p_f = video_frames[0]\n",
    "        validation_preds.append(p_a)\n",
    "\n",
    "        for i in tqdm(range(1, len(video_annotations)), desc='Validating'):\n",
    "            c_a = video_annotations[i]\n",
    "            c_f = video_frames[i]\n",
    "            p_c, c_c, bbox, scale, crop = self.data.make_crops(p_f, c_f, p_a, c_a)\n",
    "            p_c = torch.div(torch.from_numpy(p_c), 255).unsqueeze(0).float()\n",
    "            c_c = torch.div(torch.from_numpy(c_c), 255).unsqueeze(0).float()\n",
    "            bbox = torch.tensor(bbox, requires_grad=False).float()\n",
    "            preds = self.model(p_c, c_c)\n",
    "\n",
    "            del p_c\n",
    "            del c_c\n",
    "            gc.collect()\n",
    "\n",
    "            loss = torch.nn.functional.l1_loss(preds, bbox)\n",
    "            if self.comet:\n",
    "                self.comet.log_metric('val_loss', loss)\n",
    "            validation_loss.append(loss.item())\n",
    "            preds = self.data.get_bbox(preds.cpu().detach().numpy()[0], self.image_size, scale, crop)\n",
    "            validation_preds.append(preds)\n",
    "            p_a = c_a\n",
    "            p_f = c_f\n",
    "        return validation_loss, validation_preds\n",
    "\n",
    "    def train(self, epochs: int, batch_size: int, validate: bool = True, test: bool = True):\n",
    "        \"\"\"\n",
    "        Trains the model for @epochs number of epochs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs: int\n",
    "            Number of epochs to train the model\n",
    "        batch_size: int\n",
    "            The size of each batch when training the model\n",
    "        validate: bool, default=True\n",
    "            If True, validation occurs at the end of each epoch\n",
    "            The results are saved in @logs_path and models are saved in @models_path\n",
    "        test: bool, default=True\n",
    "            If True, the model is tested for tracking at the end of the training procedure\n",
    "            The results are saved in @logs_path\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            list: List containing the training loss at the end of each epoch\n",
    "        \"\"\"\n",
    "        if self.comet:\n",
    "            self.comet.log_parameter('epochs', epochs)\n",
    "            self.comet.log_parameter('batch_size', batch_size)\n",
    "        loss_per_epoch = []\n",
    "        preds_per_epoch = []\n",
    "        # Set the model to training mode\n",
    "        self.model.train()\n",
    "        # Create a DataLoader to feed data to the model\n",
    "        dataloader = torch.utils.data.DataLoader(dataset=self.data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Run for @epochs number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            if self.comet:\n",
    "                self.comet.log_metric('epoch', epoch)\n",
    "            running_loss = []\n",
    "            for step, data in enumerate(tqdm(dataloader,\n",
    "                                             total=int(len(self.data) / batch_size),\n",
    "                                             desc='Epoch {}'.format(epoch))):\n",
    "                loss = self.__train_step__(data)\n",
    "                running_loss.append(loss.item())\n",
    "            training_loss = sum(running_loss) / len(running_loss)\n",
    "            if self.comet:\n",
    "                self.comet.log_metric('mean_train_loss', training_loss)\n",
    "            loss_per_epoch.append(sum(running_loss) / len(running_loss))\n",
    "            if validate:\n",
    "                validation_loss, validation_preds = self.__validate__()\n",
    "                if self.comet:\n",
    "                    self.comet.log_metric('mean_validation_loss', validation_loss)\n",
    "                preds_per_epoch.append(validation_preds)\n",
    "                print('Validation loss: {}'.format(sum(validation_loss) / len(validation_loss)))\n",
    "            # Save the model at this stage\n",
    "            if self.models_path:\n",
    "                (self.models_path / self.model_name).mkdir(exist_ok=True)\n",
    "                torch.save(self.model, (self.models_path / self.model_name / 'epoch_{}'.format(epoch)).resolve())\n",
    "            print('Training Loss: {}'.format(training_loss))\n",
    "        # Save the validation frames, ground truths and predictions at this stage\n",
    "        if self.logs_path:\n",
    "            (self.logs_path / self.model_name).mkdir(exist_ok=True)\n",
    "            save = {'frames': self.data.video_frames[-1],\n",
    "                    'truth': self.data.video_annotations[-1],\n",
    "                    'preds': preds_per_epoch}\n",
    "            np.save(str((self.logs_path / self.model_name / 'preds_per_epoch.npy').resolve()), save)\n",
    "        # Test the model and save the results\n",
    "        if test:\n",
    "            test_loss, test_preds = self.__test__()\n",
    "            if self.logs_path:\n",
    "                (self.logs_path / self.model_name).mkdir(exist_ok=True)\n",
    "                save = {'frames': self.data.video_frames[-1],\n",
    "                        'truth': self.data.video_annotations[-1],\n",
    "                        'preds': test_preds,\n",
    "                        'loss': test_loss}\n",
    "                np.save(str((self.logs_path / self.model_name / 'test_preds.npy').resolve()), save)\n",
    "        return loss_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQMW7ZX_lThf",
    "outputId": "c939c4b5-5d7d-4368-cb0b-f41f073be53f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 950
    },
    "id": "xOAde3U_SFyK",
    "outputId": "6c047b94-af44-47c4-9b21-8e7c6a3c9358"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the path to load data\n",
    "data_path = Path('../plant-data')\n",
    "# Set the path to save predictions\n",
    "logs_path = Path('/logs')\n",
    "# Set the path to save models\n",
    "models_path = Path('../models')\n",
    "\n",
    "# Modify the hyperparamters as needed\n",
    "learning_rate = 1e-5\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "image_size = 225\n",
    "comet_api = 'BznUqlKMJs3tCDblJ9RnB7TiC'\n",
    "\n",
    "# Create an experiment object\n",
    "goturn_exp = Experiment(learning_rate=learning_rate,\n",
    "                        image_size=image_size,\n",
    "                        data_path=data_path,\n",
    "                        augment=False,\n",
    "                        logs_path=logs_path,\n",
    "                        models_path=models_path,\n",
    "                        comet_api=comet_api)\n",
    "\n",
    "# Train the model\n",
    "losses = goturn_exp.train(epochs, batch_size, validate=True, test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAad4dscn_jK",
    "outputId": "08aca42a-a35a-4c48-f969-3240e9145dd3"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NpzFile' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d485d220ede3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpath_to_validation_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../models/2022-05-20-15-02-19/epoch_49'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mpreds_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_to_validation_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_per_epoch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'frames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds_per_epoch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'truth'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NpzFile' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path_to_validation_results = Path('../models/2022-05-20-15-02-19/epoch_49') \n",
    "preds_per_epoch = np.load(path_to_validation_results.resolve()).reshape(1, )[0]\n",
    "frames = preds_per_epoch['frames']\n",
    "truth = preds_per_epoch['truth']\n",
    "preds = preds_per_epoch['preds']\n",
    "\n",
    "use_centers = False\n",
    "\n",
    "imgs = []\n",
    "for f, t, p in zip(frames, truth, preds[-1]):\n",
    "    img = f.copy()\n",
    "    img = cv2.rectangle(img, (t[0], t[1]), (t[2], t[3]), (255, 255, 255), 2)\n",
    "    img = cv2.rectangle(img, (p[0], p[1]), (p[2], p[3]), (255, 0, 0), 2)\n",
    "    img = cv2.rectangle(img, (p[0], p[1]), (p[2], p[3]), (255, 0, 0), 2)\n",
    "    imgs.append(img)\n",
    "    cv2.imshow('Tracking', img)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "height, width, channels = imgs[0].shape\n",
    "video = cv2.VideoWriter('video.avi', cv2.VideoWriter_fourcc(*\"MJPG\"), 60, (width, height))\n",
    "for img in imgs:\n",
    "    video.write(img)\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPd1x1wwcU2g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Plant-tracker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
